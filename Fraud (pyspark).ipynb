{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1566e7f-55bb-4107-8253-bf15f4e29926",
   "metadata": {},
   "source": [
    "# ðŸ’³ Financial Transaction Monitoring System â€“ PySpark Edition\n",
    "\n",
    "This PySpark-based notebook simulates 50,000+ financial transactions and applies enterprise-grade fraud detection logic including geo-anomaly detection, transaction velocity checks, and dynamic risk scoring. Built using Apache Spark to handle scale and distributed data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6b2017-312c-4214-8ed1-01b38b9aec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set python executable path\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5604efa-7afe-451d-a866-6b7a2ed8ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad1a40-af81-4f5d-a9f9-338c7b6fd953",
   "metadata": {},
   "source": [
    "# Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd52a960-1d25-4830-8cbd-2a82b2b24e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lag, unix_timestamp, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045320f9-4145-401a-8bed-8ec336f40c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FraudDetection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19beec68980>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf5937-5b22-44ee-bc37-51754c732b46",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6086b3-4fd1-4ad7-b91c-a3533ad96a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+--------+--------+--------+\n",
      "|transaction_id|           timestamp|account_id|merchant|  amount|location|\n",
      "+--------------+--------------------+----------+--------+--------+--------+\n",
      "|     TXN356652|2025-04-06 18:31:...|   ACC4277|  Amazon|10132.22|  Mumbai|\n",
      "|     TXN786016|2025-04-25 04:55:...|   ACC9218|  Amazon|13814.53|  Berlin|\n",
      "|     TXN614384|2025-04-27 10:31:...|   ACC2390| Netflix|   350.9|  Berlin|\n",
      "|     TXN704476|2025-04-29 05:02:...|   ACC3600|  Zomato|11244.37|   Tokyo|\n",
      "|     TXN189813|2025-04-23 11:36:...|   ACC5608| Netflix| 2410.97|  Berlin|\n",
      "+--------------+--------------------+----------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "cities = ['Delhi', 'Mumbai', 'New York', 'Tokyo', 'Berlin']\n",
    "merchants = ['Amazon', 'Walmart', 'Netflix', 'Uber', 'Zomato']\n",
    "\n",
    "def generate_transaction():\n",
    "    return {\n",
    "        'transaction_id': f'TXN{random.randint(100000, 999999)}',\n",
    "        'timestamp': datetime.now() - timedelta(minutes=random.randint(0, 43200)),\n",
    "        'account_id': f'ACC{random.randint(1000, 9999)}',\n",
    "        'merchant': random.choice(merchants),\n",
    "        'amount': round(random.uniform(10, 15000), 2),\n",
    "        'location': random.choice(cities)\n",
    "    }\n",
    "\n",
    "df_pd = pd.DataFrame([generate_transaction() for _ in range(50000)])\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark = df_spark.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291226a-2f49-42e8-853d-aff7605539a8",
   "metadata": {},
   "source": [
    "# Add Lag Columns for Previous Location + Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f40086-abb7-4ba4-970e-5cf61909552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"account_id\").orderBy(\"timestamp\")\n",
    "\n",
    "df_spark = df_spark \\\n",
    "    .withColumn(\"prev_location\", lag(\"location\").over(window)) \\\n",
    "    .withColumn(\"prev_time\", lag(\"timestamp\").over(window)) \\\n",
    "    .withColumn(\"time_diff_min\", \n",
    "        (unix_timestamp(\"timestamp\") - unix_timestamp(\"prev_time\")) / 60\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d332c4-5ee8-47d5-9e83-c3b2153deb63",
   "metadata": {},
   "source": [
    "# Define Fraud Rules + Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf611cfd-bb30-4e4e-ba54-720dcdd1adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark \\\n",
    "    .withColumn(\"geo_anomaly\", \n",
    "        when(\n",
    "            (col(\"location\") != col(\"prev_location\")) & (col(\"time_diff_min\") < 10), lit(True)\n",
    "        ).otherwise(lit(False))\n",
    "    ) \\\n",
    "    .withColumn(\"risk_score\", \n",
    "        when(col(\"amount\") > 5000, 30).otherwise(0) +\n",
    "        when(~col(\"location\").isin(\"Delhi\", \"Mumbai\"), 30).otherwise(0) +\n",
    "        when(col(\"geo_anomaly\") == True, 40).otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb929803-99e2-4673-ba71-8734e5385288",
   "metadata": {},
   "source": [
    "# Show High-Risk Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460e6d3-cd53-4eee-906d-c3ab1e1076e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk = df_spark.filter(col(\"risk_score\") > 70).cache()\n",
    "#print(f\"ðŸš¨ High-Risk Transaction Count: {high_risk.count()}\")\n",
    "high_risk.select(\"transaction_id\", \"account_id\", \"location\", \"amount\", \"risk_score\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819aa72c-9921-4232-952f-4cb288995d2e",
   "metadata": {},
   "source": [
    "# Save to CSV (Simulate Redshift/Snowflake Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4de29e-e1b8-438f-95f5-4478bf4ff731",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk.coalesce(1).write.csv(\"high_risk_transactions_output\", header=True, mode=\"overwrite\")\n",
    "print(\"âœ… High-risk transactions saved to 'high_risk_transactions_output/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93715912-fb3d-4ea6-8c42-76c384d2a35a",
   "metadata": {},
   "source": [
    "# (Optional) Convert for Matplotlib Graphing (via Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3733-2931-42c3-8f62-26bb4c6f00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_spark.select(\"risk_score\").toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df_plot[\"risk_score\"], bins=20, color=\"red\", kde=True)\n",
    "plt.title(\"Fraud Risk Score Distribution\")\n",
    "plt.xlabel(\"Risk Score (0â€“100)\")\n",
    "plt.ylabel(\"Transaction Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d2284-6568-4082-819c-70a5c127ec2a",
   "metadata": {},
   "source": [
    "## âœ… Conclusion\n",
    "\n",
    "- 50,000+ transactions simulated in-memory and converted to Spark DataFrame\n",
    "- Advanced rule-based fraud scoring logic applied (geo-anomaly, velocity, cross-border)\n",
    "- Risk scoring model (0â€“100) generated and filtered\n",
    "- Saved high-risk results for dashboarding or alert integration\n",
    "- Ready for pipeline orchestration using Spark on AWS Glue, EMR, or Databricks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
